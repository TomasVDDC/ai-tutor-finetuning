from transformers import AutoTokenizer


tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-2-7b-hf")
tokenizer.pad_token_id = tokenizer.eos_token_id
tokenizer.pad_token = tokenizer.eos_token
tokenizer.model_max_length = 512

vocab_indices = [  518, 29901,  8449,   278,   278,   310,   278,  2183, 10159,  1746,
        29889, 29871,  2183,  9228, 29889,  3390,   297,   278,  8093, 29889,
           13, 29985, 29906,   718, 29871, 29906, 29916, 29985, 29946,   718,
        29871, 29985, 29906,   718, 29871, 29906, 29916,   718,   278, 29918,
        29906, 29889,    13,    13,  5856, 29901,    13, 29909, 29889, 29871,
        29896, 29892,    13, 29933, 29889, 29871, 29896, 29892, 29871,    13,
        29907, 29889, 29871, 29906, 29892, 29871, 29871,    13, 29928, 29889,
        29871, 29896, 29892, 29896, 29871,    13,    13, 22550, 29901,   518,
        29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966,
        29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966,
        29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966,
        29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966,
        29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966,
        29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29961, 29966,
        29966, 29966, 29966, 29961, 29961, 29966, 29961, 29961, 29966, 29966,
        29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966,
        29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966,
        29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966,
        29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966,
        29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966,
        29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966,
        29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966,
        29966, 29966, 29966, 29966, 29961, 29966, 29966, 29966, 29966, 29966,
        29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966,
        29966, 29966, 29966, 29966, 29966, 29966, 29966, 29961, 29966, 29966,
        29966, 29966, 29966, 29961, 29966, 29966, 29966, 29966, 29966, 29966,
        29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966,
        29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966,
        29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966,
        29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966,
        29966, 29961, 29961, 29966, 29966, 29966, 29966, 29966, 29961, 29961,
        29966, 29966, 29966, 29966, 29966, 29966, 29961, 29966, 29966, 29966,
        29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29961, 29966,
        29966, 29966, 29966, 29966, 29961, 29966, 29966, 29966, 29966, 29966,
        29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966,
        29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966,
        29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966,
        29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966,
        29966, 29966, 29966, 29966, 29966, 29961, 29961, 29966, 29966, 29966,
        29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966,
        29966, 29966, 29966, 29966, 29966, 29966, 29966, 29966, 29961, 29961,
        29961, 29961, 29961, 29961, 29961, 29961, 29961, 29961, 29961, 29961,
        29961, 29961, 29961, 29966, 29966, 29966, 29966, 29966, 29961, 29961,
        29961, 29961, 29961, 29961, 29961, 29961, 29961, 29961, 29961, 29961,
        29961, 29961, 29961, 29961, 29961, 29961, 29961, 29961, 29966, 29966,
        29966, 29966, 29961, 29961, 29961, 29966, 29966, 29966, 29961, 29961,
        29961, 29961, 29961, 29961, 29961, 29961, 29961, 29961, 29961, 29966,
        29961, 29961, 29961, 29966, 29966, 29961, 29966, 29966, 29961, 29961,
        29966, 29966, 29961, 29966, 29966, 29961, 29961, 29961, 29961, 29961,
        29961, 29961, 29961, 29961, 29961, 29961, 29961, 29961, 29961, 29961,
        29961, 29961, 29961, 29961, 29961, 29961, 29961, 29961, 29961, 29961,
        29961, 29961]

attention_mask = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
        0, 0, 0, 0, 0, 0, 0, 0]

tokens = [tokenizer.decode([index]) for index in vocab_indices]


def get_last_index(attention_mask):
   for i in range(len(attention_mask) - 1, -1, -1):
      if attention_mask[i] == 1:
         print("index" , i)
         return i
      
index = get_last_index(attention_mask)

print("Indices:", vocab_indices)
print("Tokens:", tokens)
print("Prediction: ", tokens[index-1:-1])
